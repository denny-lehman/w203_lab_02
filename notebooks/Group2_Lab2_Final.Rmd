---
title: "Lab 2: Exploratory Data Analysis and Causal Model Bulding"
author: 'Team 2: Savita Chari, Denny Lehman, Tymon Silva'
date: "December 7, 2021"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load packages, message = FALSE}
library(tidyverse)
library(ggplot2) 
library(lmtest)
library(sandwich)
library(stargazer)
library(magrittr)
library(sandwich)
library(gridExtra)

```

# Exploratory Data Analysis (EDA) 

In the Pacific Northwest, drought and an increase in human water consumption have caused the most severe forest fires in centuries. For our group - this is personal - each one of us has been personally affected by recent fires. But not all forest fires are the same - atmospheric weather and moisture content in the forest floor will affect the size of the burn. In this causality project, we hope to uncover the conditions that limit the severity of forest fires with regression modeling. If we could predict the conditions that cause large fires we can better prepare our communities.

"Your introduction should present a research question and explain the concept that you're attempting to measure and how it will be operationalized. This section should pave the way for the body of the report, preparing the reader to understand why the models are constructed the way that they are. It is not enough to simply say, "We are looking for product features that enhance product success."  Your introduction must do work for you, focusing the reader on a specific measurement goal, making them care about it, and propelling the narrative forward. This is also a good time to put your work into context, discuss cross-cutting issues, and assess the overall appropriateness of the data."

# Research Question

How might we predict the size of a forest fire based on wind, rain, flammability of forest litter, flammability of deeper/larger ground forest objects, and the type fuel available to burn on the forest floor.


# Data

```{r load data} 
fire_data <- read_csv(file = '../src/data/forestfires.csv')
```


# Split the data into training and testing sets
We split the data into train-test subset to estimate the performance of our model because we had a decent size  dataset.
We kept 30% data for the training set and 70% for the test/validation data set.

```{r}
sample_size = floor(0.7*nrow(fire_data))
set.seed(777)

# randomly split data in r
picked = sample(seq_len(nrow(fire_data)),size = sample_size)
fire = fire_data[picked,]
training = fire_data[-picked,]
```

```{r}

fire
training

fire <- fire %>%
  mutate(
    rain_binary = case_when(
      rain > 0 ~ 1,
      rain == 0 ~ 0,
      )
  )

training <- training %>%
  mutate(
    rain_binary = case_when(
      rain > 0 ~ 1,
      rain == 0 ~ 0,
      )
  )


```

```{r EDA} 
# str(training)
summary(training$wind)
summary(training$rain)
summary(training$temp)
summary(training$FFMC)
summary(training$DMC)
summary(training$DC)
summary(training$area)

# unique(training$wind)
# unique(training$rain)
unique(training$temp)
range(training$temp)
# unique(training$FFMC)
# unique(training$DMC)
unique(training$DC)
range(training$DC)
# unique(training$area)

# subset(training, wind > 9)
# subset(training, area > 500)
# subset(training, area < 20)
# subset(training, area < 5)

hist(training$wind)
hist(fire_data$rain)
hist(training$temp)
hist(fire$temp)
hist(training$FFMC)
hist(fire$FFMC)
hist(training$DMC)
hist(training$DC)
hist(fire$DC)
hist(training$area)
hist(training$RH)

```

```{r removing outliers}
wind_IQR <- IQR(training$wind)
rain_IQR <- IQR(training$rain)
FFMC_IQR <- IQR(training$FFMC)
DMC_IQR <- IQR(training$DMC)
area_IQR <- IQR(training$area)

wind_outlier_upper <- 5.4 + (1.5*wind_IQR)
wind_outlier_lower <- 2.7 - (1.5*wind_IQR)

rain_outlier_upper <- 0 + (1.5*rain_IQR)
rain_outlier_lower <- 0 - (1.5*rain_IQR)

FFMC_outlier_upper <- 93.03 + (1.5*FFMC_IQR)
FFMC_outlier_lower <- 90.2 - (1.5*FFMC_IQR)

DMC_outlier_upper <- 141.57 + (1.5*DMC_IQR)
DMC_outlier_lower <- 49.95 - (1.5*DMC_IQR)

area_outlier_upper <- 8.613 + (1.5*area_IQR)
area_outlier_lower <- 0 - (1.5*area_IQR)

training <- subset(training, wind <= wind_outlier_upper & wind >= wind_outlier_lower)
training <- subset(training, rain <= rain_outlier_upper & rain >= rain_outlier_lower)
training <- subset(training, FFMC <= FFMC_outlier_upper & FFMC >= FFMC_outlier_lower)
training <- subset(training, DMC <= DMC_outlier_upper & DMC >= DMC_outlier_lower)
training <- subset(training, area <= area_outlier_upper & area >= area_outlier_lower)
# training

hist(training$wind)
hist(fire_data$rain)
hist(training$FFMC)
hist(training$DMC)
hist(training$area)

```

```{r plotting predictors vs outcome variable} 

a <- ggplot(training, aes(x = wind, y = area)) +
  geom_point() +
  geom_smooth() +
  ggtitle("wind vs area")

b <- ggplot(training, aes(x = rain_binary, y = area)) +
  geom_point() +
  geom_smooth() +
  ggtitle("rain vs area")

c <- ggplot(training, aes(x = FFMC, y = area)) +
  geom_point() +
  geom_smooth() +
  ggtitle("FFMC vs area")

d <- ggplot(training, aes(x = DMC, area)) +
  geom_point() +
  geom_smooth() +
  ggtitle("DMC vs area")

d <- ggplot(training, aes(x = temp, log(area+1))) +
  geom_point() +
  geom_smooth() +
  ggtitle("temp vs area")

# ggplot(training, aes(x = log(DC), log(area+1))) +
#   geom_point() +
#   geom_smooth() +
#   ggtitle("DMC vs area")

ggplot(training, aes(x = DC, log(area+1))) +
  geom_point() +
  geom_smooth() +
  ggtitle("DMC vs area")



grid.arrange(a, b, c, d, nrow = 2)
```

```{r plotting predictors vs log transformed outcome variable}


a <- ggplot(training, aes(x = wind, y = log(area+1))) +
  geom_point() +
  geom_smooth() +
  ggtitle("wind vs log(area+1)")

b <- ggplot(training, aes(x = rain_binary, y = log(area+1))) +
  geom_point() +
  geom_smooth() +
  ggtitle("rain vs log(area+1)")

c <- ggplot(training, aes(x = FFMC, y = log(area+1))) +
  geom_point() +
  geom_smooth() +
  ggtitle("FFMC vs log(area+1)")

d <- ggplot(training, aes(x = DMC, y = log(area+1))) +
  geom_point() +
  geom_smooth() +
  ggtitle("DMC vs log(area+1)")

grid.arrange(a, b, c, d, nrow = 2)
```

```{r plotting predictors vs sqrt transformed outcome variable }
a <- ggplot(training, aes(x = wind, y = sqrt(area))) +
  geom_point() +
  geom_smooth() +
  ggtitle("wind vs sqrt(area)")

b <- ggplot(training, aes(x = rain, y = sqrt(area))) +
  geom_point() +
  geom_smooth() +
  ggtitle("rain vs sqrt(area)")

c <- ggplot(training, aes(x = FFMC, y = sqrt(area))) +
  geom_point() +
  geom_smooth() +
  ggtitle("FFMC vs sqrt(area)")

d <- ggplot(training, aes(x = DMC, y = sqrt(area))) +
  geom_point() +
  geom_smooth() +
  ggtitle("DMC vs sqrt(area)")

grid.arrange(a, b, c, d, nrow = 2)

```

```{r plotting predictors vs cubic root transformed outcome variable }
a <- ggplot(training, aes(x = wind, y = (area)^(1/3))) +
  geom_point() +
  geom_smooth() +
  ggtitle("wind vs (area)^(1/3)")

b <- ggplot(training, aes(x = rain, y = (area)^(1/3))) +
  geom_point() +
  geom_smooth() +
  ggtitle("rain vs (area)^(1/3)")

c <- ggplot(training, aes(x = FFMC, y = (area)^(1/3))) +
  geom_point() +
  geom_smooth() +
  ggtitle("FFMC vs (area)^(1/3)")

d <- ggplot(training, aes(x = DMC, y = (area)^(1/3))) +
  geom_point() +
  geom_smooth() +
  ggtitle("DMC vs (area)^(1/3)")

grid.arrange(a, b, c, d, nrow = 2)
```
```{r plotting trasonform predictors with outcome variable }
a <- ggplot(training, aes(x = wind, y = log(area+1))) +
  geom_point() +
  geom_smooth() +
  ggtitle("wind vs log(area+1)")

b <- ggplot(training, aes(x = log(rain+1), y = log(area+1))) +
  geom_point() +
  geom_smooth() +
  ggtitle("log(rain+1) vs log(area+1)")

c <- ggplot(training, aes(x = log(FFMC), y = log(area+1))) +
  geom_point() +
  geom_smooth() +
  ggtitle("log(FFMC) vs log(area+1)")

d <- ggplot(training, aes(x = log(DMC), y = log(area+1))) +
  geom_point() +
  geom_smooth() +
  ggtitle("log(DMC) vs log(area+1)")

grid.arrange(a, b, c, d, nrow = 2)

```


# Research Design

After you have presented the introduction and the concepts that are under investigation, what data are you going to use to answer the questions? What type of research design are you using? What type of models are you going to estimate, and what goals do you have for these models?  

1. *What do you want to measure*? Make sure you identify one, or a few, variables that will allow you to derive conclusions relevant to your research question, and include those variables in all model specifications. How are the variables that you will be modeling distributed? Provide enough context and information about your data for your audience to understand whatever model results you will eventually present. 
2. What [covariates](https://en.wikipedia.org/wiki/Dependent_and_independent_variables#Statistics_synonyms) help you achieve your modeling goals? Are there problematic covariates?  either due to *collinearity*, or because they will absorb some of a causal effect you want to measure?
3. What *transformations*, if any, should you apply to each variable? These transformations might reveal linearities in the data, make our results relevant, or help us meet model assumptions.
4. Are your choices supported by exploratory data analysis (*EDA*)? You will likely start with some general EDA to *detect anomalies* (missing values, top-coded variables, etc.). From then on, your EDA should be interspersed with your model building. Use visual tools to *guide* your decisions. You can also leverage statistical *tests* to help assess whether variables, or groups of variables, are improving model fit.

At the same time, it is important to remember that you are not trying to create one perfect model. You will create several specifications, giving the reader a sense of how robust (or sensitive) your results are to modeling choices, and to show that you're not just cherry-picking the specification that leads to the largest effects.

At a minimum, you need to estimate at least three model specifications: 

The first model you include should include *only the key variables* you want to measure. These variables might be transformed, as determined by your EDA, but the model should include the absolute minimum number of covariates (usually zero or one covariate that is so crucial it would be unreasonable to omit it).

Additional models should each be defensible, and should continue to tell the story of how product features contribute to product success. This might mean including additional right-hand side features to remove omitted variable bias identified by your casual theory; or, instead, it might mean estimating a model that examines a related concept of success, or a model that investigates a heterogeneous effect. These models, and your modeling process should be defensible, incremental, and clearly explained at all points.

Your goal is to choose models that encircle the space of reasonable modeling choices, and to give an overall understanding of how these choices impact results.


```{r model building}

model1 <- lm(log(area+1) ~ wind + rain_binary + temp + FFMC + log(DMC) + DC, data = fire)

model2 <- lm(log(area+1) ~ wind + rain_binary + temp + FFMC + log(DMC) + DC + RH, data = fire)

model3 <- lm(log(area+1) ~ wind + rain_binary + temp + RH, data = fire)

# model2 <- lm(log(area+1) ~ wind + rain_binary + temp + RH + FFMC + DMC + DC + ISI, data = training)

# model3 <- lm(log(area+1) ~ wind + rain_binary + FFMC + DMC, data = training)

model_SAV_long <- lm(log(area+1) ~ rain_binary + temp + wind, data = training)
# coeftest(model_long, vcov=vcovHAC)
coeftest(model_SAV_long, vcov=vcovHAC)




# model2 <- lm(log(area+1) ~ log(ISI) , data = training)
# summary(model1)
coeftest(model1, vcov = vcovHC)

coeftest(model2, vcov = vcovHC)

coeftest(model3, vcov = vcovHC)


set.seed(5600)
shapiro.test(sample(model_SAV_long$residuals, size = 5000,replace=TRUE))


plot(model1)


```
# 4. Results

You should display all of your model specifications in a regression table, using a package like [`stargazer`](https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf) to format your output. It should be easy for the reader to find the coefficients that represent key effects near the top of the regression table, and scan horizontally to see how they change from specification to specification. Make sure that you display the most appropriate standard errors in your table.

In your text, comment on both *statistical significance and practical significance*. You may want to include statistical tests besides the standard t-tests for regression coefficients. Here, it is important that you make clear to your audience the practical significance of any model results. How should the product change as a result of what you have discovered? Are there limits to how much change you are proposing? What are the most important results that you have discovered, and what are the least important? 

### 5. Limitations of your Model 

#### 5a. Statistical limitations of your model

As a team, evaluate all of the large sample model assumptions. However, you do not necessarily want to discuss every assumption in your report. Instead, highlight any assumption that might pose significant problems for your analysis. For any violations that you identify, describe the statistical consequences. If you are able to identify any strategies to mitigate the consequences, explain these strategies. 

Note that you may need to change your model specifications in response to violations of the large sample model. 

#### 5b. Structural limitations of your model

What are the most important *omitted variables* that you were not able to measure and include in your analysis? For each variable you name, you should *reason about the direction of bias* caused by omitting this variable and whether the omission of this variable calls into question the core results you are reporting. What data could you collect that would resolve any omitted variables bias? 

# 7. Conclusion

Make sure that you end your report with a discussion that distills key insights from your estimates and addresses your research question. 
